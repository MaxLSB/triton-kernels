{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Addition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of this exercise is to understand how to build a triton kernel that adds two vectors of floating point numbers. We will compare its compute time with the vanilla pytorch addition operation.\n",
    "\n",
    "A triton kernel is a function that is executed on a GPU. The kernel is executed by multiple threads in parallel, where each thread will add a different pair of elements from the two vectors and write the result to a specific location in the output vector.\n",
    "\n",
    "The kernel is launched with a grid of blocks (in our examaple each block will have a single thread) where each block contains a number of threads. The kernel is then executed by all threads in all blocks in parallel.\n",
    "\n",
    "As an example, let's consider a vector of length 256 and a block size of 64, there would be 256 / 64 = 4 blocks (in our case 4 threads), where each of these instances of the add_kernel would access and compute the respective ranges: $[0:64], [64:128], [128:192],$ and $[192:256]$ of the vectors. And each of these instances (threads) would write the result to their respective ranges in the output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit # triton decorator that tells triton that this function is a triton function.\n",
    "def add_kernel(\n",
    "    x_ptr,  # *Pointer* to first input vector.\n",
    "    y_ptr,  # *Pointer* to second input vector.\n",
    "    output_ptr,  # *Pointer* to output vector (it needs to be preallocated).\n",
    "    n_elements,  # Size of the vector.\n",
    "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process (`constexpr` so it can be used as a shape value).\n",
    "):\n",
    "    # Since add_kernel has multiple instance we get the program id to know which part of the vector to process.\n",
    "    # For instance: Program 0: pid = 0, Program 1: pid = 1, ...\n",
    "    pid = tl.program_id(axis=0) \n",
    "    \n",
    "    # Note that offsets is a list of pointers.\n",
    "    # If pid = 1, block_start = 1 * 64, offsets = [64, 65, 66, ..., 127].\n",
    "    block_start = pid * BLOCK_SIZE \n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE) \n",
    "\n",
    "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not a multiple of the block size.\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "\n",
    "    # Write x + y back to DRAM.\n",
    "    # Each program writes back to a different part of the output vector. So everything is concatenated at the end.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build the add function, which will launch the kernel for each thread in parallel (using a grid of blocks and threads). The kernel will add the two vectors and write the result to the output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "\n",
    "    # We need to preallocate the output.\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_cuda and y.is_cuda and output.is_cuda\n",
    "    n_elements = output.numel()\n",
    "\n",
    "    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n",
    "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
    "    # It calculates the number of blocks required for your kernel based on the total number of elements (n_elements) and the number of threads per block (BLOCK_SIZE). The result is returned as a tuple containing a single value which is the number of blocks needed to process the data.\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "\n",
    "    #  Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
    "    #  \"triton.jit\" functions can be indexed with a launch grid (which is a function) to obtain a callable GPU kernel.\n",
    "    #  Don't forget to pass meta-parameters as keywords arguments.\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still running asynchronously at this point.\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the compute time for vector addition using Triton and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton: 1.060724 ms\n",
      "Torch: 1.079082 ms\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(100000, device='cuda')\n",
    "y = torch.randn(100000, device='cuda')\n",
    "\n",
    "# Warm-up to get rid of the first run overhead. \n",
    "add(x, y)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Call your function\n",
    "start = time.time()\n",
    "output = add(x, y)\n",
    "torch.cuda.synchronize() # Wait for the GPU operations to finish\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Triton: {(end - start)*1e3:.6f} ms\")    \n",
    "\n",
    "start = time.time()\n",
    "output = x + y\n",
    "torch.cuda.synchronize() # Wait for the GPU operations to finish\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Torch: {(end - start)*1e3:.6f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives roughly the equivalent time to the torch implementation, but since it is a simple operation, it doesn't really make a difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
